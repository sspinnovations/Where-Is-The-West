{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import arcgis\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.mapping import WebMap\n",
    "from arcgis.features import SpatialDataFrame\n",
    "from arcgis.geocoding import geocode\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n",
    "from IPython.display import display\n",
    "import arcpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate list of wiki lists the boring way\n",
    "wiki_western_lists = ['https://en.wikipedia.org/wiki/List_of_Western_films_of_the_1920s','https://en.wikipedia.org/wiki/List_of_Western_films_of_the_1930s','https://en.wikipedia.org/wiki/List_of_Western_films_of_the_1940s','https://en.wikipedia.org/wiki/List_of_Western_films_1950-54','https://en.wikipedia.org/wiki/List_of_Western_films_1955-59','https://en.wikipedia.org/wiki/List_of_Western_films_of_the_1960s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 titles appended\n",
      "500 titles appended\n",
      "750 titles appended\n",
      "1000 titles appended\n",
      "1250 titles appended\n",
      "1500 titles appended\n",
      "1750 titles appended\n",
      "2000 titles appended\n",
      "2250 titles appended\n"
     ]
    }
   ],
   "source": [
    "# get that sweet, sweet HTML from the above lists\n",
    "\n",
    "links = []\n",
    "titles = []\n",
    "plots = []\n",
    "iteration = 0\n",
    "for wiki_western_list in wiki_western_lists:\n",
    "    html = requests.get(wiki_western_list)\n",
    "    \n",
    "    # stick HTML into beautiful soup text object\n",
    "    \n",
    "    b = BeautifulSoup(html.text, features = \"lxml\")\n",
    "    \n",
    "    # movie titles/links were the only thing italicized with <i> tag, so we will 'find_all' on 'i'\n",
    "    for i in b.find_all(name = 'i'):\n",
    "            \n",
    "        # now that we have the line we want, we can pull just the link using 'href'\n",
    "        for link in i.find_all('a', href=True):\n",
    "            links.append(link['href'])\n",
    "            \n",
    "            # also grab the title for later\n",
    "            titles.append(link['title'])\n",
    "            iteration += 1\n",
    "            if iteration % 250 == 0:\n",
    "                print(str(iteration) + ' titles appended')\n",
    "\n",
    "# not all these movies have wiki articles, so we'll nuke those from our big list                        \n",
    "links_existing = [x for x in links if \"redlink\" not in x]\n",
    "\n",
    "titles_existing = [i for i in titles if '(page does not exist)' not in i]\n",
    "            \n",
    "# have to make the full url from what we grabbed\n",
    "wiki_western_links = ['https://en.wikipedia.org' + i for i in links_existing]\n",
    "wiki_western_titles = titles_existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Western Links: 1995\n",
      "Number of Western Titles: 1995\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Western Links: {len(wiki_western_links)}')\n",
    "print(f'Number of Western Titles: {len(wiki_western_titles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_western_pages = list(zip(wiki_western_titles, wiki_western_links))\n",
    "len(wiki_western_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 plots appended\n",
      "500 plots appended\n",
      "750 plots appended\n",
      "1000 plots appended\n",
      "1250 plots appended\n",
      "1500 plots appended\n",
      "1750 plots appended\n"
     ]
    }
   ],
   "source": [
    "# create list to handle loose wiki standards\n",
    "# great article about this stuff on Medium.com by This Time Is Different\n",
    "possibles = ['Plot','Synopsis','Plot synopsis','Plot summary', \n",
    "             'Story','Plotline','The Beginning','Summary',\n",
    "             'Content','Premise']\n",
    "possibles_edit = [i + 'Edit' for i in possibles]\n",
    "all_possibles = possibles + possibles_edit\n",
    "# go ask wikipedia for page info using the titles we put together\n",
    "iteration = 0\n",
    "exception = 0\n",
    "for t in wiki_western_pages:\n",
    "    iteration += 250\n",
    "    if iteration % 10 == 0:\n",
    "        print(str(iteration) + ' plots appended')\n",
    "    wik = wikipedia.WikipediaPage(t[0])\n",
    "    \n",
    "    # is there plot info of some sort?\n",
    "    try:\n",
    "        for p in all_possibles:\n",
    "            if wik.section(p) != None:\n",
    "                \n",
    "                # add whatever we find to plots list\n",
    "                plot = wik.section(p).replace('\\n','').replace(\"\\'\",\"\")\n",
    "                plots.append(plot)\n",
    "                \n",
    "    # handle exceptions\n",
    "    except:\n",
    "        exception += 1\n",
    "        if exception % 250 == 0:\n",
    "            print(str(exception) + ' exceptions skipped')\n",
    "        plot = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 entities appended\n",
      "500 entities appended\n",
      "750 entities appended\n",
      "1000 entities appended\n",
      "1250 entities appended\n",
      "1500 entities appended\n",
      "1750 entities appended\n",
      "2000 entities appended\n",
      "2250 entities appended\n",
      "2500 entities appended\n",
      "2750 entities appended\n",
      "3000 entities appended\n",
      "3250 entities appended\n",
      "3500 entities appended\n",
      "3750 entities appended\n"
     ]
    }
   ],
   "source": [
    "# run plots through a natural language processor to find geopolitical entities\n",
    "# you first need to download/install en_core_web_sm at https://spacy.io/models/en\n",
    "nlp = en_core_web_sm.load()\n",
    "gpe = []\n",
    "iteration = 0\n",
    "appendnum = 0\n",
    "for plot in plots:\n",
    "    doc = nlp(plot)\n",
    "    for ent in doc.ents:\n",
    "        if (ent.label_ == 'GPE'):\n",
    "            gpe.append(ent.text)\n",
    "            appendnum += 1\n",
    "            if appendnum % 250 == 0:\n",
    "                print(str(appendnum) + ' entities appended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shudak\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-SF-migration\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\shudak\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-SF-migration\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 entities not in wiki\n",
      "250 entities rejected\n",
      "500 entities accepted\n",
      "750 entities accepted\n",
      "500 entities not in wiki\n",
      "500 entities rejected\n",
      "750 entities not in wiki\n",
      "1250 entities accepted\n",
      "750 entities rejected\n",
      "1500 entities accepted\n",
      "1000 entities not in wiki\n",
      "1750 entities accepted\n"
     ]
    }
   ],
   "source": [
    "# we want to query wikipedia to clean up our list\n",
    "# if the wiki summary returned from a search on the geopolitical entities has a city-like word AND a western State we keep it\n",
    "addresses = []\n",
    "valid_gpe_list = ['city', 'municipality', 'capital', 'town', 'village', 'census']\n",
    "# continental USA west of the mississippi\n",
    "valid_state_list = ['Arizona','California','Colorado','Idaho','Montana','Nevada','New Mexico','Oregon',\n",
    "                    'Utah','Washington','Wyoming','North Dakota','South Dakota','Nebraska','Kansas',\n",
    "                    'Oklahoma','Texas','Minnesota','Iowa','Missouri','Arkansas','Louisiana']\n",
    "dice = 0\n",
    "no_dice = 0\n",
    "lost = 0\n",
    "\n",
    "for e in gpe:\n",
    "    try: \n",
    "        summary = str(wikipedia.summary(e))\n",
    "\n",
    "        if any(v in summary for v in valid_gpe_list):\n",
    "            if any(s in summary for s in valid_state_list):\n",
    "                addresses.append(e + ', United States')\n",
    "                dice += 1\n",
    "                if dice % 250 == 0:\n",
    "                    print(str(dice) + ' entities accepted')\n",
    "            else:\n",
    "                no_dice += 1\n",
    "        else:\n",
    "            no_dice += 1\n",
    "            if no_dice % 250 == 0:\n",
    "                print(str(no_dice) + ' entities rejected')\n",
    "\n",
    "    except:\n",
    "        lost += 1\n",
    "        if lost % 250 == 0:\n",
    "            print(str(lost) + ' entities not in wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 addresses geocoded\n",
      "500 addresses geocoded\n",
      "750 addresses geocoded\n",
      "1000 addresses geocoded\n"
     ]
    }
   ],
   "source": [
    "# create dataframe for locations\n",
    "# geocode entities to get xy's\n",
    "# append entities and xy's to dataframe\n",
    "gis = GIS()\n",
    "df = pd.DataFrame()\n",
    "iteration = 0\n",
    "geocode_pass = 0\n",
    "for address in addresses:\n",
    "    try:\n",
    "        location = geocode(address)[0]\n",
    "        df = df.append({'address': address[:-5], 'x_column': location['attributes']['DisplayX'],'y_column': location['attributes']['DisplayY']},ignore_index=True)\n",
    "        iteration += 1\n",
    "        if iteration % 250 == 0:\n",
    "            print(str(iteration) + ' addresses geocoded')\n",
    "    except:\n",
    "        geocode_pass += 1\n",
    "        if geocode_pass % 250 == 0:\n",
    "            print(str(geocode_pass) + ' geocodes passed')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to spatially enabled dataframe\n",
    "wsdf = pd.DataFrame.spatial.from_xy(df, 'x_column', 'y_column', sr=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dumb things we found after looking at the data points\n",
    "drop_list = ['US, United S', 'United S', 'U.S., United S', 'America, United S', 'the United States, United S','United States, United S','United States of America, United S']\n",
    "wsdf = wsdf[~wsdf['address'].isin(drop_list)]\n",
    "mdf = wsdf.copy()\n",
    "#mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-106.2693241648988, 36.35037924014938)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf.spatial.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f02deb1385a4b87a2a97c976d0be884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MapView(layout=Layout(height='400px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"map-static-img-preview-eec8abc8-e5c3-4972-93d0-ff38ba009ca8\"><img src=\"\"></img></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m1 = GIS().map('United States')\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.zoom = 4\n",
    "m1.center = [39,-98]\n",
    "layer = mdf.spatial.to_feature_collection()\n",
    "m1.add_layer(layer, {\"renderer\":\"HeatmapRenderer\",\"opacity\":0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf.spatial.plot(map_widget=m1,\n",
    "        symbol_type='simple',\n",
    "        symbol_style='s',\n",
    "        cmap='Blues_r',\n",
    "        cstep=35,\n",
    "        outline_color='binary',\n",
    "        marker_size=5,\n",
    "        line_width=.5,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
